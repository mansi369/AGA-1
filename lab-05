# prompt: Implement a deep learning model using stacked RBMs.
# Train layer-wise and fine-tune using backpropagation.
# Compare performance with traditional deep networks.

import numpy as np
from sklearn.neural_network import BernoulliRBM
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.datasets import fetch_openml
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA


# Step 1: Load and preprocess the dataset (e.g., MNIST)
def load_data():
    mnist = fetch_openml("mnist_784", version=1)
    X = mnist.data / 255.0
    y = mnist.target.astype(int)

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    pca = PCA(n_components=100)  # Reduce dimensionality using PCA
    X = pca.fit_transform(X)
    return X, y


# Step 2: Define the stacked RBM architecture
def create_stacked_rbm(n_layers, n_units_per_layer):
    rbms = []
    for i in range(n_layers):
        rbm = BernoulliRBM(n_components=n_units_per_layer, learning_rate=0.01, batch_size=10, n_iter=10, random_state=0)
        rbms.append(rbm)
    return rbms


# Step 3: Train the stacked RBM layer-wise
def train_stacked_rbm(rbms, X_train):
    X_transformed = X_train.copy()
    for i, rbm in enumerate(rbms):
        print(f"Training layer {i + 1}/{len(rbms)}...")
        rbm.fit(X_transformed)
        X_transformed = rbm.transform(X_transformed)
    return rbms


# Step 4: Fine-tune with backpropagation (using logistic regression as an example)
def fine_tune(rbms, X_train, y_train, X_test, y_test):
    # Transform the training and testing data using the trained stacked RBM
    X_train_transformed = X_train.copy()
    for rbm in rbms:
        X_train_transformed = rbm.transform(X_train_transformed)

    X_test_transformed = X_test.copy()
    for rbm in rbms:
        X_test_transformed = rbm.transform(X_test_transformed)


    # Train a classifier on the transformed data
    classifier = LogisticRegression(max_iter=1000)
    classifier.fit(X_train_transformed, y_train)

    # Evaluate the classifier
    y_pred = classifier.predict(X_test_transformed)
    report = classification_report(y_test, y_pred)
    print("Classification report after fine-tuning:\n", report)

    return classifier

#Step 5: Train a traditional deep network for comparison

def train_deep_network(X_train, y_train, X_test, y_test):
    classifier = LogisticRegression(max_iter=1000)
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
    report = classification_report(y_test, y_pred)
    print("Classification report of traditional deep network:\n", report)
    return classifier



if __name__ == "__main__":
    # Load data
    X, y = load_data()
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

    # Create stacked RBM
    n_layers = 2
    n_units_per_layer = 100  # Adjust this for better performance
    rbms = create_stacked_rbm(n_layers, n_units_per_layer)

    # Train stacked RBM
    rbms = train_stacked_rbm(rbms, X_train)

    # Fine-tune with backpropagation (Logistic Regression)
    classifier_stacked_rbm = fine_tune(rbms, X_train, y_train, X_test, y_test)

    classifier_deep_network = train_deep_network(X_train, y_train, X_test, y_test)
